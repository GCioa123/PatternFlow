{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "G_Chaudhari VQVAE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPXp5fGKNqGAa9nhlEGlSGv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GCioa123/PatternFlow/blob/topic-recognition/G_Chaudhari_VQVAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTb4lLSuWDRe"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_probability as tfp\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import load_img"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6rAw4V2IYA8",
        "outputId": "bbda16c3-0691-4af9-d6b2-bc2df90d9cd4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xep4-xdNMMj"
      },
      "source": [
        "import os\n",
        "\n",
        "#Loading the data\n",
        "\n",
        "input_train_dir = 'gdrive/My Drive/keras_png_slices_data/keras_png_slices_data/keras_png_slices_train/' #this you have give your seg_train folder path\n",
        "target_train_dir = 'gdrive/My Drive/keras_png_slices_data/keras_png_slices_data/keras_png_slices_seg_train/' #seg_val folder path\n",
        "input_val_dir = 'gdrive/My Drive/keras_png_slices_data/keras_png_slices_data/keras_png_slices_validate/' #this you have give your seg_train folder path\n",
        "target_val_dir = 'gdrive/My Drive/keras_png_slices_data/keras_png_slices_data/keras_png_slices_seg_validate/' #seg_val folder path\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mocffPMdPb_-",
        "outputId": "80b7ac4d-91dd-4c59-fa46-a261a12a413f"
      },
      "source": [
        "# Extract data to train and test data sets\n",
        "img_size = (256, 256)\n",
        "num_classes = 4\n",
        "batch_size = 1\n",
        "\n",
        "input_img_paths = sorted(\n",
        "    [\n",
        "        os.path.join(input_train_dir, fname)\n",
        "        for fname in os.listdir(input_train_dir)\n",
        "        if fname.endswith(\".png\")\n",
        "    ]\n",
        ")\n",
        "target_img_paths = sorted(\n",
        "    [\n",
        "        os.path.join(target_train_dir, fname)\n",
        "        for fname in os.listdir(target_train_dir)\n",
        "        if fname.endswith(\".png\") and not fname.startswith(\".\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "val_input_img_paths = sorted(\n",
        "    [\n",
        "        os.path.join(input_val_dir, fname)\n",
        "        for fname in os.listdir(input_val_dir)\n",
        "        if fname.endswith(\".png\")\n",
        "    ]\n",
        ")\n",
        "val_target_img_paths = sorted(\n",
        "    [\n",
        "        os.path.join(target_val_dir, fname)\n",
        "        for fname in os.listdir(target_val_dir)\n",
        "        if fname.endswith(\".png\") and not fname.startswith(\".\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"Number of samples:\", len(input_img_paths))\n",
        "\n",
        "# for input_path, target_path in zip(input_img_paths[:10], target_img_paths[:10]):\n",
        "#     print(input_path, \"|\", target_path)"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 9664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PClDjJN7Ncxi"
      },
      "source": [
        "#vectoriser\n",
        "\n",
        "class VectorQuantizer(layers.Layer):\n",
        "    def __init__(self, num_embeddings, embedding_dim, beta=0.25, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.beta = (\n",
        "            beta  # This parameter is best kept between [0.25, 2] as per the paper.\n",
        "        )\n",
        "\n",
        "        # Initialize the embeddings which we will quantize.\n",
        "        w_init = tf.random_uniform_initializer()\n",
        "        self.embeddings = tf.Variable(\n",
        "            initial_value=w_init(\n",
        "                shape=(self.embedding_dim, self.num_embeddings), dtype=\"float32\"\n",
        "            ),\n",
        "            trainable=True,\n",
        "            name=\"embeddings_vqvae\",\n",
        "        )\n",
        "\n",
        "    def call(self, x):\n",
        "        # Calculate the input shape of the inputs and\n",
        "        # then flatten the inputs keeping `embedding_dim` intact.\n",
        "        input_shape = tf.shape(x)\n",
        "        flattened = tf.reshape(x, [-1, self.embedding_dim])\n",
        "\n",
        "        # Quantization.\n",
        "        encoding_indices = self.get_code_indices(flattened)\n",
        "        encodings = tf.one_hot(encoding_indices, self.num_embeddings)\n",
        "        quantized = tf.matmul(encodings, self.embeddings, transpose_b=True)\n",
        "        quantized = tf.reshape(quantized, input_shape)\n",
        "\n",
        "        # Calculate vector quantization loss and add that to the layer. You can learn more\n",
        "        # about adding losses to different layers here:\n",
        "        # https://keras.io/guides/making_new_layers_and_models_via_subclassing/. Check\n",
        "        # the original paper to get a handle on the formulation of the loss function.\n",
        "        commitment_loss = self.beta * tf.reduce_mean(\n",
        "            (tf.stop_gradient(quantized) - x) ** 2\n",
        "        )\n",
        "        codebook_loss = tf.reduce_mean((quantized - tf.stop_gradient(x)) ** 2)\n",
        "        self.add_loss(commitment_loss + codebook_loss)\n",
        "\n",
        "        # Straight-through estimator.\n",
        "        quantized = x + tf.stop_gradient(quantized - x)\n",
        "        return quantized\n",
        "\n",
        "    def get_code_indices(self, flattened_inputs):\n",
        "        # Calculate L2-normalized distance between the inputs and the codes.\n",
        "        similarity = tf.matmul(flattened_inputs, self.embeddings)\n",
        "        distances = (\n",
        "            tf.reduce_sum(flattened_inputs ** 2, axis=1, keepdims=True)\n",
        "            + tf.reduce_sum(self.embeddings ** 2, axis=0)\n",
        "            - 2 * similarity\n",
        "        )\n",
        "\n",
        "        # Derive the indices for minimum distances.\n",
        "        encoding_indices = tf.argmin(distances, axis=1)\n",
        "        return encoding_indices"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23s4Qw1dNant"
      },
      "source": [
        "#encoder \n",
        "def get_encoder(latent_dim=32):\n",
        "    encoder_inputs = keras.Input(shape=(28, 28, 3))\n",
        "    x = layers.Conv2D(16, 3, activation=\"relu\", strides=2, padding=\"same\")(\n",
        "        encoder_inputs\n",
        "    )\n",
        "    x = layers.Conv2D(16, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "    encoder_outputs = layers.Conv2D(latent_dim, 3, padding=\"same\")(x)\n",
        "    return keras.Model(encoder_inputs, encoder_outputs, name=\"encoder\")\n"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8K2fEzwNep0"
      },
      "source": [
        "#decoder\n",
        "def get_decoder(latent_dim=32):\n",
        "    latent_inputs = keras.Input(shape=get_encoder().output.shape[1:])\n",
        "    # print(latent_inputs.shape)\n",
        "    x = layers.Conv2DTranspose(16,3, activation=\"relu\", strides=2, padding=\"same\")(\n",
        "        latent_inputs\n",
        "    )\n",
        "    x = layers.Conv2DTranspose(16, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "    decoder_outputs = layers.Conv2DTranspose(3, 3, padding=\"same\")(x)\n",
        "    return keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "my_o9U8PNgZ7",
        "outputId": "b5b3fb0e-dfa3-46f3-9cc7-e977e43fa03f"
      },
      "source": [
        "#Stand-alone VQ-VAE\n",
        "def get_vqvae(latent_dim=32, num_embeddings=64):\n",
        "    vq_layer = VectorQuantizer(num_embeddings, latent_dim, name=\"vector_quantizer\")\n",
        "    encoder = get_encoder(latent_dim)\n",
        "    decoder = get_decoder(latent_dim)\n",
        "    inputs = keras.Input(shape=(28, 28, 3))\n",
        "    encoder_outputs = encoder(inputs)\n",
        "    quantized_latents = vq_layer(encoder_outputs)\n",
        "    reconstructions = decoder(quantized_latents)\n",
        "    return keras.Model(inputs, reconstructions, name=\"vq_vae\")\n",
        "\n",
        "\n",
        "get_vqvae().summary()\n"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vq_vae\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_116 (InputLayer)      [(None, 28, 28, 3)]       0         \n",
            "                                                                 \n",
            " encoder (Functional)        (None, 7, 7, 32)          7408      \n",
            "                                                                 \n",
            " vector_quantizer (VectorQua  (None, 7, 7, 32)         2048      \n",
            " ntizer)                                                         \n",
            "                                                                 \n",
            " decoder (Functional)        (None, 28, 28, 3)         7379      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 16,835\n",
            "Trainable params: 16,835\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdRnn0YhPM_f"
      },
      "source": [
        "#VQ-VAE Trainer\n",
        "class VQVAETrainer(keras.models.Model):\n",
        "    def __init__(self, train_variance, latent_dim=32, num_embeddings=128, **kwargs):\n",
        "        super(VQVAETrainer, self).__init__(**kwargs)\n",
        "        self.train_variance = train_variance\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_embeddings = num_embeddings\n",
        "\n",
        "        self.vqvae = get_vqvae(self.latent_dim, self.num_embeddings)\n",
        "\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\"\n",
        "        )\n",
        "        self.vq_loss_tracker = keras.metrics.Mean(name=\"vq_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.vq_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def train_step(self, x):\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Outputs from the VQ-VAE.\n",
        "            # og code delete - reconstructions = self.vqvae(x)\n",
        "            # x = tf.expand_dims(x, axis=1)\n",
        "\n",
        "            tmp_result = self.vqvae(x) #change tmp\n",
        "\n",
        "            #Calculate image difference using SSIM\n",
        "  \n",
        "            # print(x.shape)\n",
        "            # print(tf.expand_dims(x,-1).shape)\n",
        "            # print(tmp_result.shape)\n",
        "            #change\n",
        "            # print(\"x\",x)\n",
        "            # print(\"o/p\",tmp_result)\n",
        "            \n",
        "\n",
        "            img_diff = tf.image.ssim(x,tmp_result,1.0)\n",
        "            \n",
        "            # print(\"img\",img_diff)\n",
        "\n",
        "\n",
        "            # Calculate the losses. Mean squared error, change\n",
        "            reconstruction_loss = (\n",
        "                tf.reduce_mean((x - tmp_result) ** 2) / self.train_variance\n",
        "            )\n",
        "            total_loss = ( img_diff + sum(self.vqvae.losses))\n",
        "\n",
        "        # Backpropagation.\n",
        "        grads = tape.gradient(total_loss, self.vqvae.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.vqvae.trainable_variables))\n",
        "        #self.optimizer = tf.keras.optimizers.Adam(0.001).minimize((-1*img_diff))\n",
        "\n",
        "        # Loss tracking.\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.vq_loss_tracker.update_state(sum(self.vqvae.losses))\n",
        "\n",
        "        # Log results.\n",
        "        return {\n",
        "            \"loss(SSIM)\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"vqvae_loss\": self.vq_loss_tracker.result(),\n",
        "        }\n"
      ],
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1t8EXjOZAFrO"
      },
      "source": [
        ""
      ],
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jukvz4ZVRMKZ"
      },
      "source": [
        "\n",
        "class oasis_data(keras.utils.Sequence):\n",
        "    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n",
        "\n",
        "    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths):\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.input_img_paths = input_img_paths\n",
        "        self.target_img_paths = target_img_paths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.target_img_paths) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
        "        i = idx * self.batch_size\n",
        "        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n",
        "        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n",
        "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n",
        "        for j, path in enumerate(batch_input_img_paths):\n",
        "            img = np.array(load_img(path, target_size=self.img_size))\n",
        "            x[j] = img/255\n",
        "        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"uint8\")\n",
        "        for j, path in enumerate(batch_target_img_paths):\n",
        "            img = np.array(load_img(path, target_size=self.img_size, color_mode=\"grayscale\"))\n",
        "            # y[j] = np.expand_dims(img, 2)\n",
        "            # # Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n",
        "            # y[j] -= 1\n",
        "            one_hot = img == [0, 85, 170, 255]\n",
        "            y[j] = one_hot\n",
        "            \n",
        "        \n",
        "        #print(y)\n",
        "        # return x, y\n",
        "        return x\n"
      ],
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3OsroXMQjsk"
      },
      "source": [
        "#train, test, split \n",
        "\n",
        "import random\n",
        "\n",
        "# Split our img paths into a training and a validation set\n",
        "\n",
        "val_samples = 1000\n",
        "# random.Random(1337).shuffle(input_img_paths)\n",
        "# random.Random(1337).shuffle(target_img_paths)\n",
        "train_input_img_paths = input_img_paths[:]\n",
        "train_target_img_paths = target_img_paths[:]\n",
        "# val_input_img_paths = input_img_paths[-val_samples:]\n",
        "# val_target_img_paths = target_img_paths[-val_samples:]\n",
        "val_input_img_paths = val_input_img_paths[:]\n",
        "val_target_img_paths = val_target_img_paths[:]\n",
        "\n",
        "# Instantiate data Sequences for each split\n",
        "#train data\n",
        "train_gen = oasis_data(\n",
        "    batch_size, img_size, train_input_img_paths, train_target_img_paths\n",
        ")\n",
        "#test data\n",
        "val_gen =  oasis_data(batch_size, img_size, val_input_img_paths, val_target_img_paths)"
      ],
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhKQDfd3FSkO",
        "outputId": "359b06f9-b1c7-4ae0-9ec9-66165967e2fa"
      },
      "source": [
        "print(train_gen.__len__())\n"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yk_Vhc6JTKWp",
        "outputId": "00d2c5aa-7a7b-4156-a663-c157fa2755b1"
      },
      "source": [
        "# X_train,y_train = train_gen.__getitem__(9664)\n",
        "# X_test, y_test = val_gen.__getitem__(1) .reshape(256,256,3)\n",
        "X_train = np.array([tf.reshape(tf.image.resize(train_gen.__getitem__(i),[28,28]),[28,28,3]) for i in range(train_gen.__len__())])\n",
        "X_test = np.array([val_gen.__getitem__(i) for i in range(5)])\n",
        "# X_train = np.array([tf.image.resize(train_gen.__getitem__(i),[28,28]) for i in range(5)])\n",
        "\n",
        "\n",
        "# dimension reduction to make images the same shape\n",
        "# X_train = tf.expand_dims(X_train, axis=1)\n",
        "# X_test = tf.expand_dims(X_test, axis=0) \n",
        "\n",
        "x_train_scaled = (X_train / 255.0) - 0.5\n",
        "x_test_scaled = (X_test / 255.0) - 0.5\n",
        "\n",
        "data_variance = np.var(X_train / 255.0)\n",
        "\n",
        "print(data_variance)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fea573Vb4oyJ",
        "outputId": "40c9784f-a5d9-449f-be4c-4670f38e49d8"
      },
      "source": [
        "#tf.image.ssim(X_train[0], X_train[1],1)\n",
        "print(X_train[0].shape)"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28, 28, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0YdlzMfJQcH",
        "outputId": "c4c73f59-7b26-4b3c-daf9-78791f40fefa"
      },
      "source": [
        "# temp = X_train[0].reshape(256,256,3)\n",
        "len(X_train)"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EX4EcS9Wvog",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ac61292-95f6-4ae7-8cfc-6436711dfc9b"
      },
      "source": [
        "#train VQ VAE model\n",
        "vqvae_trainer = VQVAETrainer(data_variance, latent_dim=32, num_embeddings=128)\n",
        "vqvae_trainer.compile(optimizer=keras.optimizers.Adam())\n",
        "vqvae_trainer.fit(x_train_scaled, epochs=50)\n"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "4/4 [==============================] - 2s 101ms/step - loss(SSIM): 0.0020 - reconstruction_loss: 506275.3438 - vqvae_loss: 0.0069\n",
            "Epoch 2/50\n",
            "4/4 [==============================] - 0s 101ms/step - loss(SSIM): -0.0447 - reconstruction_loss: 482646.8125 - vqvae_loss: 0.0104\n",
            "Epoch 3/50\n",
            "4/4 [==============================] - 0s 99ms/step - loss(SSIM): -0.0917 - reconstruction_loss: 453122.7812 - vqvae_loss: 0.0224\n",
            "Epoch 4/50\n",
            "4/4 [==============================] - 0s 100ms/step - loss(SSIM): -0.1391 - reconstruction_loss: 410265.5938 - vqvae_loss: 0.0564\n",
            "Epoch 5/50\n",
            "4/4 [==============================] - 0s 93ms/step - loss(SSIM): -0.1591 - reconstruction_loss: 353320.0000 - vqvae_loss: 0.1438\n",
            "Epoch 6/50\n",
            "4/4 [==============================] - 0s 101ms/step - loss(SSIM): -0.0867 - reconstruction_loss: 287666.8750 - vqvae_loss: 0.3562\n",
            "Epoch 7/50\n",
            "4/4 [==============================] - 0s 98ms/step - loss(SSIM): 0.2325 - reconstruction_loss: 217011.6250 - vqvae_loss: 0.8545\n",
            "Epoch 8/50\n",
            "4/4 [==============================] - 0s 100ms/step - loss(SSIM): 1.1484 - reconstruction_loss: 144698.1406 - vqvae_loss: 1.9749\n",
            "Epoch 9/50\n",
            "4/4 [==============================] - 0s 101ms/step - loss(SSIM): 3.2602 - reconstruction_loss: 79357.1250 - vqvae_loss: 4.2499\n",
            "Epoch 10/50\n",
            "4/4 [==============================] - 0s 98ms/step - loss(SSIM): 5.7599 - reconstruction_loss: 35683.2891 - vqvae_loss: 6.6845\n",
            "Epoch 11/50\n",
            "4/4 [==============================] - 0s 101ms/step - loss(SSIM): 4.9775 - reconstruction_loss: 19924.3574 - vqvae_loss: 5.8115\n",
            "Epoch 12/50\n",
            "4/4 [==============================] - 0s 99ms/step - loss(SSIM): 2.4480 - reconstruction_loss: 21243.1621 - vqvae_loss: 3.2967\n",
            "Epoch 13/50\n",
            "4/4 [==============================] - 0s 102ms/step - loss(SSIM): 0.5988 - reconstruction_loss: 23796.5078 - vqvae_loss: 1.4835\n",
            "Epoch 14/50\n",
            "4/4 [==============================] - 0s 97ms/step - loss(SSIM): -0.3186 - reconstruction_loss: 21376.6094 - vqvae_loss: 0.6007\n",
            "Epoch 15/50\n",
            "4/4 [==============================] - 0s 96ms/step - loss(SSIM): -0.7057 - reconstruction_loss: 17355.4102 - vqvae_loss: 0.2426\n",
            "Epoch 16/50\n",
            "4/4 [==============================] - 0s 97ms/step - loss(SSIM): -0.8584 - reconstruction_loss: 16177.3340 - vqvae_loss: 0.1091\n",
            "Epoch 17/50\n",
            "4/4 [==============================] - 0s 99ms/step - loss(SSIM): -0.8824 - reconstruction_loss: 25396.0664 - vqvae_loss: 0.0597\n",
            "Epoch 18/50\n",
            "4/4 [==============================] - 0s 98ms/step - loss(SSIM): -0.8701 - reconstruction_loss: 35135.6094 - vqvae_loss: 0.0395\n",
            "Epoch 19/50\n",
            "4/4 [==============================] - 0s 99ms/step - loss(SSIM): -0.8666 - reconstruction_loss: 44498.9492 - vqvae_loss: 0.0302\n",
            "Epoch 20/50\n",
            "4/4 [==============================] - 0s 102ms/step - loss(SSIM): -0.8626 - reconstruction_loss: 48701.9023 - vqvae_loss: 0.0252\n",
            "Epoch 21/50\n",
            "4/4 [==============================] - 0s 92ms/step - loss(SSIM): -0.8787 - reconstruction_loss: 49532.3281 - vqvae_loss: 0.0223\n",
            "Epoch 22/50\n",
            "4/4 [==============================] - 0s 98ms/step - loss(SSIM): -0.7837 - reconstruction_loss: 54382.5391 - vqvae_loss: 0.0204\n",
            "Epoch 23/50\n",
            "4/4 [==============================] - 0s 94ms/step - loss(SSIM): -0.6022 - reconstruction_loss: 60546.7812 - vqvae_loss: 0.0193\n",
            "Epoch 24/50\n",
            "4/4 [==============================] - 0s 96ms/step - loss(SSIM): -0.8190 - reconstruction_loss: 52958.1680 - vqvae_loss: 0.0171\n",
            "Epoch 25/50\n",
            "4/4 [==============================] - 0s 109ms/step - loss(SSIM): -0.8942 - reconstruction_loss: 40747.1328 - vqvae_loss: 0.0159\n",
            "Epoch 26/50\n",
            "4/4 [==============================] - 0s 95ms/step - loss(SSIM): -0.9041 - reconstruction_loss: 28944.3262 - vqvae_loss: 0.0150\n",
            "Epoch 27/50\n",
            "4/4 [==============================] - 0s 105ms/step - loss(SSIM): -0.9206 - reconstruction_loss: 20600.3281 - vqvae_loss: 0.0130\n",
            "Epoch 28/50\n",
            "4/4 [==============================] - 0s 98ms/step - loss(SSIM): -0.9427 - reconstruction_loss: 16395.6934 - vqvae_loss: 0.0108\n",
            "Epoch 29/50\n",
            "4/4 [==============================] - 0s 104ms/step - loss(SSIM): -0.9500 - reconstruction_loss: 14125.1621 - vqvae_loss: 0.0087\n",
            "Epoch 30/50\n",
            "4/4 [==============================] - 0s 101ms/step - loss(SSIM): -0.9511 - reconstruction_loss: 13218.7246 - vqvae_loss: 0.0075\n",
            "Epoch 31/50\n",
            "4/4 [==============================] - 0s 93ms/step - loss(SSIM): -0.9540 - reconstruction_loss: 12518.3320 - vqvae_loss: 0.0065\n",
            "Epoch 32/50\n",
            "4/4 [==============================] - 0s 102ms/step - loss(SSIM): -0.9569 - reconstruction_loss: 13234.3389 - vqvae_loss: 0.0057\n",
            "Epoch 33/50\n",
            "4/4 [==============================] - 0s 102ms/step - loss(SSIM): -0.9650 - reconstruction_loss: 13515.7490 - vqvae_loss: 0.0049\n",
            "Epoch 34/50\n",
            "4/4 [==============================] - 0s 101ms/step - loss(SSIM): -0.9699 - reconstruction_loss: 14924.9375 - vqvae_loss: 0.0043\n",
            "Epoch 35/50\n",
            "4/4 [==============================] - 0s 99ms/step - loss(SSIM): -0.9604 - reconstruction_loss: 16414.3789 - vqvae_loss: 0.0038\n",
            "Epoch 36/50\n",
            "4/4 [==============================] - 0s 94ms/step - loss(SSIM): -0.9671 - reconstruction_loss: 16860.7070 - vqvae_loss: 0.0034\n",
            "Epoch 37/50\n",
            "4/4 [==============================] - 0s 102ms/step - loss(SSIM): -0.9727 - reconstruction_loss: 17305.9531 - vqvae_loss: 0.0031\n",
            "Epoch 38/50\n",
            "4/4 [==============================] - 0s 97ms/step - loss(SSIM): -0.9759 - reconstruction_loss: 16984.0254 - vqvae_loss: 0.0028\n",
            "Epoch 39/50\n",
            "4/4 [==============================] - 0s 99ms/step - loss(SSIM): -0.9770 - reconstruction_loss: 16319.3008 - vqvae_loss: 0.0026\n",
            "Epoch 40/50\n",
            "4/4 [==============================] - 0s 106ms/step - loss(SSIM): -0.9779 - reconstruction_loss: 15340.9492 - vqvae_loss: 0.0023\n",
            "Epoch 41/50\n",
            "4/4 [==============================] - 0s 96ms/step - loss(SSIM): -0.9803 - reconstruction_loss: 14126.0488 - vqvae_loss: 0.0021\n",
            "Epoch 42/50\n",
            "4/4 [==============================] - 0s 103ms/step - loss(SSIM): -0.9823 - reconstruction_loss: 13053.3086 - vqvae_loss: 0.0018\n",
            "Epoch 43/50\n",
            "4/4 [==============================] - 0s 101ms/step - loss(SSIM): -0.9838 - reconstruction_loss: 12064.8047 - vqvae_loss: 0.0017\n",
            "Epoch 44/50\n",
            "4/4 [==============================] - 0s 97ms/step - loss(SSIM): -0.9849 - reconstruction_loss: 11356.4268 - vqvae_loss: 0.0015\n",
            "Epoch 45/50\n",
            "4/4 [==============================] - 0s 108ms/step - loss(SSIM): -0.9858 - reconstruction_loss: 10846.8477 - vqvae_loss: 0.0014\n",
            "Epoch 46/50\n",
            "4/4 [==============================] - 0s 102ms/step - loss(SSIM): -0.9864 - reconstruction_loss: 10486.0879 - vqvae_loss: 0.0013\n",
            "Epoch 47/50\n",
            "4/4 [==============================] - 0s 98ms/step - loss(SSIM): -0.9869 - reconstruction_loss: 10240.9453 - vqvae_loss: 0.0013\n",
            "Epoch 48/50\n",
            "4/4 [==============================] - 0s 101ms/step - loss(SSIM): -0.9873 - reconstruction_loss: 10066.5928 - vqvae_loss: 0.0012\n",
            "Epoch 49/50\n",
            "4/4 [==============================] - 0s 102ms/step - loss(SSIM): -0.9858 - reconstruction_loss: 9949.9141 - vqvae_loss: 0.0012\n",
            "Epoch 50/50\n",
            "4/4 [==============================] - 0s 101ms/step - loss(SSIM): -0.9867 - reconstruction_loss: 9867.7090 - vqvae_loss: 0.0012\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f35379f6e90>"
            ]
          },
          "metadata": {},
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNhrEQBJ5rmZ"
      },
      "source": [
        "m = get_vqvae()"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Kf6GabqPM54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "39ae2199-96c9-44fb-8d6a-d9561d0a506b"
      },
      "source": [
        "m(x_train_scaled[1])"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-139-a2f603f95019>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_scaled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mspec_dim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mspec_dim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\u001b[0m\u001b[1;32m    264\u001b[0m                              \u001b[0;34m'incompatible with the layer: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                              \u001b[0;34mf'expected shape={spec.shape}, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"vq_vae\" is incompatible with the layer: expected shape=(None, 28, 28, 3), found shape=(28, 28, 3)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJcgUrEdWe1P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58d7098f-ff31-46a7-d7c8-9b1631b8efe0"
      },
      "source": [
        "tf.image.ssim(x_train_scaled[0],x_train_scaled[1],1)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.99998206], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roYloKpY3SU3",
        "outputId": "84216cce-d8c6-4634-f099-54e42539dea8"
      },
      "source": [
        "x_train_scaled[0].shape"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1, 28, 28, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmYtSRIl6BeE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}