{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "G_Chaudhari VQVAE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO1W3D9Dq3/hn8OZZJr8eO6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GCioa123/PatternFlow/blob/topic-recognition/G_Chaudhari_VQVAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTb4lLSuWDRe"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_probability as tfp\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import load_img"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6rAw4V2IYA8",
        "outputId": "07a54421-b6a2-4cf2-9722-f19012431f73"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xep4-xdNMMj"
      },
      "source": [
        "import os\n",
        "\n",
        "#Loading the data\n",
        "\n",
        "input_train_dir = 'gdrive/My Drive/keras_png_slices_data/keras_png_slices_data/keras_png_slices_train/' #this you have give your seg_train folder path\n",
        "target_train_dir = 'gdrive/My Drive/keras_png_slices_data/keras_png_slices_data/keras_png_slices_seg_train/' #seg_val folder path\n",
        "input_val_dir = 'gdrive/My Drive/keras_png_slices_data/keras_png_slices_data/keras_png_slices_validate/' #this you have give your seg_train folder path\n",
        "target_val_dir = 'gdrive/My Drive/keras_png_slices_data/keras_png_slices_data/keras_png_slices_seg_validate/' #seg_val folder path\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mocffPMdPb_-",
        "outputId": "997d49ff-cead-49cc-ead1-730d603a3f8b"
      },
      "source": [
        "# Extract data to train and test data sets\n",
        "img_size = (256, 256)\n",
        "num_classes = 4\n",
        "batch_size = 1\n",
        "\n",
        "input_img_paths = sorted(\n",
        "    [\n",
        "        os.path.join(input_train_dir, fname)\n",
        "        for fname in os.listdir(input_train_dir)\n",
        "        if fname.endswith(\".png\")\n",
        "    ]\n",
        ")\n",
        "target_img_paths = sorted(\n",
        "    [\n",
        "        os.path.join(target_train_dir, fname)\n",
        "        for fname in os.listdir(target_train_dir)\n",
        "        if fname.endswith(\".png\") and not fname.startswith(\".\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "val_input_img_paths = sorted(\n",
        "    [\n",
        "        os.path.join(input_val_dir, fname)\n",
        "        for fname in os.listdir(input_val_dir)\n",
        "        if fname.endswith(\".png\")\n",
        "    ]\n",
        ")\n",
        "val_target_img_paths = sorted(\n",
        "    [\n",
        "        os.path.join(target_val_dir, fname)\n",
        "        for fname in os.listdir(target_val_dir)\n",
        "        if fname.endswith(\".png\") and not fname.startswith(\".\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"Number of samples:\", len(input_img_paths))\n",
        "\n",
        "# for input_path, target_path in zip(input_img_paths[:10], target_img_paths[:10]):\n",
        "#     print(input_path, \"|\", target_path)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 9664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PClDjJN7Ncxi"
      },
      "source": [
        "#vectoriser\n",
        "\n",
        "class VectorQuantizer(layers.Layer):\n",
        "    def __init__(self, num_embeddings, embedding_dim, beta=0.25, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.beta = (\n",
        "            beta  # This parameter is best kept between [0.25, 2] as per the paper.\n",
        "        )\n",
        "\n",
        "        # Initialize the embeddings which we will quantize.\n",
        "        w_init = tf.random_uniform_initializer()\n",
        "        self.embeddings = tf.Variable(\n",
        "            initial_value=w_init(\n",
        "                shape=(self.embedding_dim, self.num_embeddings), dtype=\"float32\"\n",
        "            ),\n",
        "            trainable=True,\n",
        "            name=\"embeddings_vqvae\",\n",
        "        )\n",
        "\n",
        "    def call(self, x):\n",
        "        # Calculate the input shape of the inputs and\n",
        "        # then flatten the inputs keeping `embedding_dim` intact.\n",
        "        input_shape = tf.shape(x)\n",
        "        flattened = tf.reshape(x, [-1, self.embedding_dim])\n",
        "\n",
        "        # Quantization.\n",
        "        encoding_indices = self.get_code_indices(flattened)\n",
        "        encodings = tf.one_hot(encoding_indices, self.num_embeddings)\n",
        "        quantized = tf.matmul(encodings, self.embeddings, transpose_b=True)\n",
        "        quantized = tf.reshape(quantized, input_shape)\n",
        "\n",
        "        # Calculate vector quantization loss and add that to the layer. You can learn more\n",
        "        # about adding losses to different layers here:\n",
        "        # https://keras.io/guides/making_new_layers_and_models_via_subclassing/. Check\n",
        "        # the original paper to get a handle on the formulation of the loss function.\n",
        "        commitment_loss = self.beta * tf.reduce_mean(\n",
        "            (tf.stop_gradient(quantized) - x) ** 2\n",
        "        )\n",
        "        codebook_loss = tf.reduce_mean((quantized - tf.stop_gradient(x)) ** 2)\n",
        "        self.add_loss(commitment_loss + codebook_loss)\n",
        "\n",
        "        # Straight-through estimator.\n",
        "        quantized = x + tf.stop_gradient(quantized - x)\n",
        "        return quantized\n",
        "\n",
        "    def get_code_indices(self, flattened_inputs):\n",
        "        # Calculate L2-normalized distance between the inputs and the codes.\n",
        "        similarity = tf.matmul(flattened_inputs, self.embeddings)\n",
        "        distances = (\n",
        "            tf.reduce_sum(flattened_inputs ** 2, axis=1, keepdims=True)\n",
        "            + tf.reduce_sum(self.embeddings ** 2, axis=0)\n",
        "            - 2 * similarity\n",
        "        )\n",
        "\n",
        "        # Derive the indices for minimum distances.\n",
        "        encoding_indices = tf.argmin(distances, axis=1)\n",
        "        return encoding_indices"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23s4Qw1dNant"
      },
      "source": [
        "#encoder \n",
        "def get_encoder(latent_dim=32):\n",
        "    encoder_inputs = keras.Input(shape=(256, 256, 3))\n",
        "    x = layers.Conv2D(16, 3, activation=\"relu\", strides=2, padding=\"same\")(\n",
        "        encoder_inputs\n",
        "    )\n",
        "    x = layers.Conv2D(16, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "    encoder_outputs = layers.Conv2D(latent_dim, 3, padding=\"same\")(x)\n",
        "    return keras.Model(encoder_inputs, encoder_outputs, name=\"encoder\")\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8K2fEzwNep0"
      },
      "source": [
        "#decoder\n",
        "def get_decoder(latent_dim=32):\n",
        "    latent_inputs = keras.Input(shape=get_encoder().output.shape[1:])\n",
        "    # print(latent_inputs.shape)\n",
        "    x = layers.Conv2DTranspose(16,(256,256), activation=\"relu\", strides=2, padding=\"same\")(\n",
        "        latent_inputs\n",
        "    )\n",
        "    x = layers.Conv2DTranspose(16, (256,256), activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "    decoder_outputs = layers.Conv2DTranspose(3, (256,256), padding=\"same\")(x)\n",
        "    return keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "my_o9U8PNgZ7",
        "outputId": "75b28840-b74c-4939-a036-0c66c8b3efcc"
      },
      "source": [
        "#Stand-alone VQ-VAE\n",
        "def get_vqvae(latent_dim=32, num_embeddings=64):\n",
        "    vq_layer = VectorQuantizer(num_embeddings, latent_dim, name=\"vector_quantizer\")\n",
        "    encoder = get_encoder(latent_dim)\n",
        "    decoder = get_decoder(latent_dim)\n",
        "    inputs = keras.Input(shape=(256, 256, 3))\n",
        "    encoder_outputs = encoder(inputs)\n",
        "    quantized_latents = vq_layer(encoder_outputs)\n",
        "    reconstructions = decoder(quantized_latents)\n",
        "    return keras.Model(inputs, reconstructions, name=\"vq_vae\")\n",
        "\n",
        "\n",
        "get_vqvae().summary()\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vq_vae\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
            "                                                                 \n",
            " encoder (Functional)        (None, 64, 64, 32)        7408      \n",
            "                                                                 \n",
            " vector_quantizer (VectorQua  (None, 64, 64, 32)       2048      \n",
            " ntizer)                                                         \n",
            "                                                                 \n",
            " decoder (Functional)        (None, 256, 256, 3)       53477411  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 53,486,867\n",
            "Trainable params: 53,486,867\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdRnn0YhPM_f"
      },
      "source": [
        "#VQ-VAE Trainer\n",
        "class VQVAETrainer(keras.models.Model):\n",
        "    def __init__(self, train_variance, latent_dim=32, num_embeddings=128, **kwargs):\n",
        "        super(VQVAETrainer, self).__init__(**kwargs)\n",
        "        self.train_variance = train_variance\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_embeddings = num_embeddings\n",
        "\n",
        "        self.vqvae = get_vqvae(self.latent_dim, self.num_embeddings)\n",
        "\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\"\n",
        "        )\n",
        "        self.vq_loss_tracker = keras.metrics.Mean(name=\"vq_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.vq_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def train_step(self, x):\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Outputs from the VQ-VAE.\n",
        "            # og code delete - reconstructions = self.vqvae(x)\n",
        "            tmp_result = self.vqvae(x) #change tmp\n",
        "\n",
        "            #Calculate image difference using SSIM\n",
        "  \n",
        "            # print(x.shape)\n",
        "            # print(tf.expand_dims(x,-1).shape)\n",
        "            # print(tmp_result.shape)\n",
        "            #change\n",
        "            print(\"x\",x)\n",
        "            print(\"o/p\",tmp_result)\n",
        "            \n",
        "\n",
        "            img_diff = tf.image.ssim(x, tmp_result, 3.0)\n",
        "            \n",
        "            print(img_diff)\n",
        "\n",
        "\n",
        "            # Calculate the losses. Mean squared error, change\n",
        "            reconstruction_loss = (\n",
        "                tf.reduce_mean((x - tmp_result) ** 2) / self.train_variance\n",
        "            )\n",
        "            total_loss = -1*(img_diff + sum(self.vqvae.losses))\n",
        "\n",
        "        # Backpropagation.\n",
        "        grads = tape.gradient(total_loss, self.vqvae.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.vqvae.trainable_variables))\n",
        "        #self.optimizer = tf.keras.optimizers.Adam(0.001).minimize((-1*img_diff))\n",
        "\n",
        "        # Loss tracking.\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.vq_loss_tracker.update_state(sum(self.vqvae.losses))\n",
        "\n",
        "        # Log results.\n",
        "        return {\n",
        "            \"loss(SSIM)\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"vqvae_loss\": self.vq_loss_tracker.result(),\n",
        "        }\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1t8EXjOZAFrO"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jukvz4ZVRMKZ"
      },
      "source": [
        "\n",
        "class oasis_data(keras.utils.Sequence):\n",
        "    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n",
        "\n",
        "    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths):\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.input_img_paths = input_img_paths\n",
        "        self.target_img_paths = target_img_paths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.target_img_paths) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
        "        i = idx * self.batch_size\n",
        "        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n",
        "        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n",
        "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n",
        "        for j, path in enumerate(batch_input_img_paths):\n",
        "            img = np.array(load_img(path, target_size=self.img_size))\n",
        "            x[j] = img/255\n",
        "        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"uint8\")\n",
        "        for j, path in enumerate(batch_target_img_paths):\n",
        "            img = np.array(load_img(path, target_size=self.img_size, color_mode=\"grayscale\"))\n",
        "            # y[j] = np.expand_dims(img, 2)\n",
        "            # # Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n",
        "            # y[j] -= 1\n",
        "            one_hot = img == [0, 85, 170, 255]\n",
        "            y[j] = one_hot\n",
        "            \n",
        "        \n",
        "        #print(y)\n",
        "        # return x, y\n",
        "        return x\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3OsroXMQjsk"
      },
      "source": [
        "#train, test, split \n",
        "\n",
        "import random\n",
        "\n",
        "# Split our img paths into a training and a validation set\n",
        "\n",
        "val_samples = 1000\n",
        "# random.Random(1337).shuffle(input_img_paths)\n",
        "# random.Random(1337).shuffle(target_img_paths)\n",
        "train_input_img_paths = input_img_paths[:]\n",
        "train_target_img_paths = target_img_paths[:]\n",
        "# val_input_img_paths = input_img_paths[-val_samples:]\n",
        "# val_target_img_paths = target_img_paths[-val_samples:]\n",
        "val_input_img_paths = val_input_img_paths[:]\n",
        "val_target_img_paths = val_target_img_paths[:]\n",
        "\n",
        "# Instantiate data Sequences for each split\n",
        "#train data\n",
        "train_gen = oasis_data(\n",
        "    batch_size, img_size, train_input_img_paths, train_target_img_paths\n",
        ")\n",
        "#test data\n",
        "val_gen =  oasis_data(batch_size, img_size, val_input_img_paths, val_target_img_paths)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhKQDfd3FSkO",
        "outputId": "60f9e3c1-3fa1-486a-a9fa-8ddf4637e416"
      },
      "source": [
        "print(train_gen.__len__())\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yk_Vhc6JTKWp",
        "outputId": "a376ed21-158a-47d7-f642-fe748b5c0109"
      },
      "source": [
        "# X_train,y_train = train_gen.__getitem__(9664)\n",
        "# X_test, y_test = val_gen.__getitem__(1) .reshape(256,256,3)\n",
        "X_train = np.array([np.array(train_gen.__getitem__(i)).reshape(256,256,3) for i in range(5)])\n",
        "X_test = np.array([val_gen.__getitem__(i) for i in range(5)])\n",
        "\n",
        "# dimension reduction to make images the same shape\n",
        "# X_train = tf.expand_dims(X_train, axis=0)\n",
        "# X_test = tf.expand_dims(X_test, axis=0) \n",
        "\n",
        "x_train_scaled = (X_train / 255.0) - 0.5\n",
        "x_test_scaled = (X_test / 255.0) - 0.5\n",
        "\n",
        "data_variance = np.var(X_train / 255.0)\n",
        "\n",
        "print(data_variance)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.872651e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fea573Vb4oyJ",
        "outputId": "0f34932e-9416-47e1-9831-9cd69e460b27"
      },
      "source": [
        "#tf.image.ssim(X_train[0], X_train[1],1)\n",
        "print(X_train[0].shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(256, 256, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0YdlzMfJQcH"
      },
      "source": [
        "temp = X_train[0].reshape(256,256,3)\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EX4EcS9Wvog",
        "outputId": "b4faad01-6f46-46ff-b8c0-90b89c0b936c"
      },
      "source": [
        "#train VQ VAE model\n",
        "vqvae_trainer = VQVAETrainer(data_variance, latent_dim=32, num_embeddings=256)\n",
        "vqvae_trainer.compile(optimizer=keras.optimizers.Adam())\n",
        "vqvae_trainer.fit(x_train_scaled, epochs=10, batch_size=128)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "x Tensor(\"IteratorGetNext:0\", shape=(None, 256, 256, 3), dtype=float32)\n",
            "o/p Tensor(\"vq_vae/decoder/conv2d_transpose_5/BiasAdd:0\", shape=(None, 256, 256, 3), dtype=float32)\n",
            "Tensor(\"SSIM/Mean_2:0\", shape=(None,), dtype=float32)\n",
            "x Tensor(\"IteratorGetNext:0\", shape=(None, 256, 256, 3), dtype=float32)\n",
            "o/p Tensor(\"vq_vae/decoder/conv2d_transpose_5/BiasAdd:0\", shape=(None, 256, 256, 3), dtype=float32)\n",
            "Tensor(\"SSIM/Mean_2:0\", shape=(None,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Kf6GabqPM54"
      },
      "source": [
        "type(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJcgUrEdWe1P"
      },
      "source": [
        "tf.image.ssim(X_train[0],X_train[1],1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}